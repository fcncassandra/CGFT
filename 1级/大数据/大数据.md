## 8 自然语言处理与信息检索

### 基础知识

基础知识：自然语言概念

> - 语言是一种交流的方法，我们可以说，读和写。例如，我们认为，我们使用自然语言做决定和计划，确切地说是用词。然而，在这个AI时代，我们面临的一个大问题是，我们是否可以以类似的方式与计算机通信。换句话说，人类能用自然语言与计算机交流吗？开发 NLP 应用程序对我们来说是一个挑战，因为计算机需要结构化数据，但人类语言是非结构化的。
> 
> - 从这个意义上说，我们可以说自然语言处理（NLP）是计算机科学的子领域，尤其是人工智能（AI），它关注使计算机能够理解和处理人类语言。从技术上讲，NLP的主要任务是对计算机进行编程，从而可以分析和处理大量的自然语言数据

基础知识：NLP的发展

> - 1950年前：阿兰•图灵图灵测试，人和机器进行交流，如果人无法判断自己交流的对象是人还是机器，就说明这个机器具有智能的。
> 
> - 1950-1970：主流是基于规则形式语言理论。乔姆斯基，根据数学中的公理化方法研究自然语言，采用代数和集合论把形式语言定义为符号的序列。他试图使用有限的规则描述无限的语言现象，发现人类普遍的语言机制，建立所谓的普遍语法。
> 
> - 1970-至今：主流是基于统计的研究。代表的公司有谷歌、微软、IBM。20世纪70年代，弗里德里克•贾里尼克及其领导的IBM华生实验室将语音识别率从70%提升到90%。 1988年，IBM的彼得•布朗提出了基于统计的机器翻译方法。 2005年，Google机器翻译打败基于规则的Sys Tran。
> 
> - 2010年以后：机器学习逆袭。AlphaGo先后战胜李世石、柯洁等，掀起人工智能热潮。深度学习、人工神经网络成为热词。主要领域有语音识别、图像识别、机器翻译、自动驾驶、智能家居。

基础知识：几个术语的概念

> - 自然语言处理（NLP，Natural Language Processing）：通过算法、统计或常识专门处理语言的各种方法的学科。
> 
> - 自然语言理解 （NLU，Natural Language Understanding)：对某种自然语言的文本的真正理解。
> 
> - 计算语言学（Computational Linguistic）：从语言学的角度来分析、处理自然语言，试图以机器或计算机模拟人的语言能力。目前来看，计算机语言学和自然语言处理方向一致，两者可以看作同一事物的不同名称
> 
> - NLP = CL

基础知识：语言分析的层次

> 1. 形态分析（词汇分析）Morphologic
> 2. 句法分析 Syntax 
> 3. 语义分析 Semantic
> 4. 语用分析 Pragmatics
> 5. 篇章分析（文本分析）Discourse
> 6. 世界知识分析 World

### 词汇分析

词的构成

> - 词素：词是基于最小的语义单元-**词素**构成的，如played = play+ed，cats = cat+s，unfrendly = un+friend+ly
>   
>   - 词素可以分为两种：词干+词缀
>   
>   - 词缀又可以分为两种：前缀、后缀
> 
> - 变形：变形是同一个单词的不同形式，比如原形Lemma改变：knife-knives，也可以不加词缀：mouse-mice
>   
>   - 形态学过程可以形成新词：
>     
>     - 衍生=词干+词缀 friend+ly = friendly
>     
>     - 组合=词干+词干 rail+way = railway
> 
> - 有限状态机：使用有限状态自动机(Finite State Automata)来表示词的形成过程。状态：开始（start），结束（ end），中间（ intermediate）
>   
>   - 状态之间是可以转化的；我们可以把它看成识别字符串；下面我将举例描述下如何用FSA进行单词分析
>     
>     ![](大数据.assets/2023-06-18-22-44-45-image.png)
>   
>   - 在了解了词的一般构成和分析之后，形态学分析的应用一般包括以下3个方面：词（token）、 词的原形（lemma）、词性（ part of speech）

分词

> - 定义：简单地讲，自动分词就是让计算机系统在文本中的词与词之间自动加上空格或其他边界标记。
>   
>   - 首先用正则表达式可以自己完成
>   
>   - 如果要利用已有工具，Python NLTK中的word_tokenize(),其实它也是正则表达式实现的。
> 
> - 分词主要实现以下几个功能：
>   • 将’分开 don’t -> don’t, they’ll -> they ‘ll;
>   • 将大部分标点当作单独的一个词;
>   • 将后一位是逗号或者引号的词分开;
>   • 单独出现在一行的句号分开

词形还原（stem 和 lemmatize，后者更好）

> - Stemming和 lemmatizing 不是完全相同的，stemming是去掉词缀,比如：
>   
>   playplay； replayed re-play-ed；computerizedcomput-er-ize-d
> 
> - 但是lemmatizing是找到原形，其实也就是基于变形或者衍生的不同，但是很多时候，这两种还原方式是一样的。
>   
>   ![](大数据.assets/2023-06-18-22-47-24-image.png)

词性标记

> - 标记是以句子为单位的，而不是单词为单位
> 
> - 标记缩写：
>   • NNP 单数名词（Proper noun, singular）
>   • VBD 动词过去式（Verb, past tense）
>   • IN 介词或从属连词（Preposition or subordinating conjunction）
>   
>   • CC 并列连词（Coordinating conjunction）
>   • TO to

命名实体识别

> - 从非结构化文本中抽结构化信息，而人名、地名、组织机构名、时间和数字表达（包括时间、日期、货币量和百分数等）是结构化信息的关键内容
> 
> - 命名实体识别和分类（named entity recognition and classification, NERC）任务确切地讲，就是识别这些实体指称的**边界**和**类别**。下面是斯坦福和spaCy定义的命名实体的类别。

### 句法分析

句法分析的概念

> - 1 解析器的概念
>   
>   ![](大数据.assets/2023-06-18-22-50-23-image.png)
>   
>   解析的主要内容包括：
>   • 报告语法错误。
>   • 从经常发生的错误中恢复，以便继续处理程序的其余部分。
>   • 创建解析树。
>   • 创建符号表。
>   • 生成中间表示（IR）。
> 
> - 2 派生的概念
>   
>   **为了得到输入字符串，我们需要一系列的产生式规则**。派生是一组产生式规则。在解析过程中，我们需要确定要替换的非终端，以及决定要替换的非终端的产生式规则。主要的派生方式有两种：
>   
>   - 最左端派生：在最左边的推导中，输入的句子形式从左到右被扫描和替换。
>   
>   - 最右端派生：在最左边的推导中，输入的句子形式从右到左被扫描和替换。
> 
> - 3 解析树的概念
>   
>   解析树可以定义为派生的图形描述。派生的起始符号用作解析树的根。在每个解析树中，叶节点是终端，内部节点是非终端。解析树的一个属性是可以按顺序遍历将生成原始输入字符串
> 
> - 4 语法的概念
>   
>   语法对于描述格式良好的程序的句法结构是非常必要和重要的。在文学意义上，它们表示自然语言中会话的句法规则。自从英语、印地语等自然语言诞生以来，语言学就一直试图定义语法。
>   
>   形式语言理论也适用于计算机科学领域，主要应用于编程语言和数据结构。例如，在C语言中，精确的语法规则说明了函数是如何由列表和声明构成的。语法根据解析类型可以被分为两种： 
>   
>   - **短语结构或成分结构语法**
>   
>   - **依存语法**

短语结构/成分结构语法（组成，树的枝）

> - 定义：短语结构语法是由Noam Chomsky提出的一种**基于成分关系**的语法。这就是为什么它也被称为成分语法。它与依存语法相反。
> 
> - 基本要点：成分语法和成分关系的基本要点。
>   
>   - 所有相关的框架都从成分关系的角度来看待句子结构。
>   
>   - 构成关系来源于拉丁语和希腊语的主谓划分。
>   
>   - 从名词短语NP和动词短语VP两个方面来理解基本从句结构，NP和VP后面可以进一步分解。
>     
>     ![](大数据.assets/2023-06-18-22-56-04-image.png)

依存语法（依存，图的边）

> - 定义：它与结构语法相反，是建立在依存关系基础上的。它是由Lucien Tesnier提出的。依存语法（DG）由于缺乏短语节点而与成分语法相反。
> 
> - 基本要点：依存语法和依存关系的基本点。
>   
>   - 在DG中，语言单位（即词）通过有向连接相互连接。
>   
>   - 动词成为从句结构的中心。
>   
>   - 每一个其他的句法单位都与动词有直接联系。这些语法单位被称为依存项。
>     
>     ![](大数据.assets/2023-06-18-22-57-44-image.png)

### 语义分析

语义角色标注

> - SRL（semantic role labeling）--根据一组预先定义的关系来识别和标注句子中语义谓词的角色， (比如, “who” did “what” to “whom”)
> 
> - 语义谓词，论元结构的两种形式：
>   
>   - 基于成分的，更关注短语（Constituent-based (i.e., phrase or span)）
>     例如: [ Marry ]A0  [ borrowed ]V <u> [ a book ]A1</u>  <u>[ from John ]A2</u> [ last week ]AM-TMP
>   
>   - 基于依存的，更关注词（Dependency-based）
>     例如: [ Marry ]A0  [ borrowed ]V<u>  a [ book ]A1</u>  <u>from [ John]A2</u>  last [ week ]AM-TMP
> 
> - 论元的概念：在语言学中，论元就是指一个句子中带有名词性的词。比如，Children like sweets.这是一个two-place predication,也就是含有两个论元（argument），即CHILDREN,SWEET。典型的语义论元包括主体、病人、工具等，也包括位置、时间、方式、原因等附加语。
> 
> - 四个子任务：
>   
>   - 谓词标识（make）
>   
>   - 谓词消歧（make.02）到底是哪个意思
>   
>   - 论元标识（例如，someone）有哪几个argument，这里有3个
>   
>   - 论元分类（someone是谓词make的A0）
>     
>     ![](大数据.assets/2023-06-25-20-26-00-image.png)

指代消解

> - 定义：指代消解是找出文本中名词短语所指代的真实世界中的事物。
>   
>   - 注意：不只是代词能够指代其他事物，所有格和其他名词性短语也可以。甚至还存在大量嵌套的指代。
>   
>   - 如：这里• his, he 指代 Barack Obama![](大数据.assets/2023-06-25-20-26-48-image.png)
> 
> - 应用：
>   
>   - 全文理解：完整的文章中有大量的指代
>   
>   - 机器翻译：土耳其语不区分男他和女她，翻译到英文的时候必须做指代消解。
>   
>   - 文本摘要：使用代词会使行文更加自然
>   
>   - 信息提取和QA系统：比如搜索“谁娶了Claudia Ross”，得到“He married Claudia Ross in 1971”，则系统必须消解“He”是谁。

情感分析

> - 定义：情感分析是用来识别文本的情绪。
>   
>   - 它也被用来识别情感没有明确表达的地方。很多公司正在使用情绪分析，识别在线客户的意见和情绪。它将帮助公司了解客户对产品和服务的看法。企业可以通过情感分析从客户的帖子中判断自己的整体声誉。这样，我们可以说，除了确定简单的极性之外，情感分析还可以理解上下文中的情绪，以帮助我们更好地理解所表达的观点背后的含义。
> 
> - 情感分类：
>   
>   - 情绪（emotion）：有一定原因引发的同步反应。例如悲伤（sadness），快乐（joy）
>   
>   - 心情（mood）：没有明显原因引发的长期低强度的主观感受变化。例如忧郁（gloomy），倦怠（listless）
>   
>   - 人际立场（interpersonal stance）：对他人的特定反应。例如疏远（distant），冷漠（cold）
>   
>   - <u>**态度（attitude）：对特定人或事物的带有主观色彩的偏好或倾向。喜欢（like），讨厌（hate）**</u>
>   
>   - 个性特质（personal traits）：相对稳定的个性倾向和行为趋势。例如焦虑（nervous），渴望（anxious）
> 
> - 我们所叙述的情感分析是基于态度的。内容包括：
>   
>   - （1）态度的持有者（source）
>   
>   - （2）态度的目标（aspect）
>   
>   - （3）态度的类型：一系列类型如喜欢（like），讨厌（hate），珍视（value），渴望（desire）等；或着简单的加权极性如积极（positive），消极（negative）和中性（neutral）并可用具体的权重修饰
>   
>   - （4）态度的范围：某句话/全文
> 
> - 因此情感分析的目标可以分为一下几种：
>   
>   - 初级：文章的整体感情是积极/消极的？
>   
>   - 进阶：对文章的态度从1-5打分
>   
>   - 高级：检测态度的目标，持有者和类

单词的向量化

> - 定义：word2vec是word embedding（词向量）的一种浅层神经网络训练方法。通俗的讲就是把一个词变成一个词向量。
>   
>   - 首先我们来设计一个任务，给定一个句子中的一个单词（input word），然后随机取一个这个单词上下文的单词。对于词表中的每个单词，我们随机挑一个“附近的词”，计算每个词的概率。当我们提到“附近”的时候，实际上是这个算法中的窗口大小。
> 
> - 举例：从句子”The quick brown fox jumps over the lazy dog.”中提取出来用来训练网络模型的单词对。
>   
>   - 在这个例子中，窗口大小设置为2，蓝色的单词表示输入。![](大数据.assets/2023-06-25-20-32-10-image.png)
>   
>   - 首先我们得从我们的训练文本里构建一个词典，我们假设词典大小是10000个不一样的单词。输入是某个单词，比如“ants”，在字典里会有一个one-hot 的向量对应（只有一个维度为1的向量）输出是概率，可以转化为one-hot 的向量
>     
>     ![](大数据.assets/2023-06-25-20-33-30-image.png)
>   
>   - 这个模型的隐藏层实际上就像是一个查询字典。隐藏层的输出实际上就是输入单词的词向量
>     
>     ![](大数据.assets/2023-06-25-20-35-16-image.png)

### 信息检索

信息检索

> - 定义：
>   
>   - 信息检索(IR)系统所用数据模型比数据库系统简单
>   
>   - 基于用户输入（如关键词或文档样例）来查找相关文档
>   
>   - 可用于带有描述性关键词的非文本数据（如照片）
>   
>   - 网页搜索引擎就是信息检索系统的一个实例
> 
> - 与数据库系统的区别：
>   
>   - （1） IR系统不处理事务更新（如并发控制与恢复）
>   
>   - （2） 数据库系统处理结构化数据, 具有定义数据组织的模式
>   
>   - （3） IR系统需要处理某些数据库系统通常不讨论的查询问题：
>     
>     - 根据关键词的**模糊搜索**
>     
>     - 根据**相关度估计**的检索结果排名
> 
> - **关键词搜索**：
>   
>   - 定义：在全文本(full text)检索中, 文档中的所有单词都视为关键词，用术语词项(term)来指文档中的词。IR系统利用关键词和逻辑连接词**and**, or, not来形成查询表达式，and 是隐含的，无需显式说明
>   
>   - 相关性排名基于下列因素：
>     
>     - （1）词项频率Term frequency：关键词在文档中的出现频率
>     
>     - （2）逆文档频率Inverse document frequency：关键词在多少文档中出现，如果越少就给关键词越多重要度
>     
>     - （3）指向文档的超链接Hyperlinks to documents：指向文档的链接越多，文档越重要

相关度排名

> - 1 利用词项的相关度排名：
>   
>   - TF-IDF（词项频率-逆文档频率）排名
>     
>     - n (d) = 文档 d 中词项的数目
>     
>     - n(d, t) = 词项 t 在文档 d 中的出现数目
>     
>     - n(t) = 包含词项 t 的文档数目
>       
>       ![](大数据.assets/2023-06-25-20-41-18-image.png)
>   
>   - 多数系统对上述模型进行了改进：
>     
>     - 出现在标题、作者列表、节标题等等中的词应给予较高的重要度
>     
>     - 在文档中首次出现很晚的词应给予较低的重要度
>     
>     - 删除极其常见的词，如 “a”、“an”、“the”、“it” 等，它们的IDF很低，称为停止词
>     
>     - 接近度：若查询中的多个词项在文档中紧靠在一起出现，该文档具有比它们远远分开出现时更高的重要度。
> 
> - 2 基于相关度的检索：检索与给定文档相似的文档
>   
>   - 共同词项：相关度可以根据共同词项来定义，例如，在A中找出k个具有最高TF (d, t) / n (t)的词项, 并用这些词项查询其他文档。
>   
>   - 向量空间模型：定义一个n-维空间, 其中n是文档集合中出现的词项总数
>     
>     - 文档d的向量；余弦相似度
>   
>   - 相关反馈：相似度可用来精化关键词查询结果，用户从被关键词查询返回的文档中选择一些最相关的文档, 系统再找出与这些文档相似的其他文档。
> 
> - 3 利用超链接的相关度
>   
>   - 问题：如果只考虑基于词项的相关度（TF-IDF）与一个查询的关键词相关的网页数目可能是巨大的，容易造成“搜索引擎作弊（spamming）”例如一个旅游网站可在其网页中加入许多词“travel”的出现，从而使其网页针对关键词travel的排位很高
>   
>   - 大多数时候人们喜欢访问热门站点的网页，可以采用的几种方法
>     
>     - （1） 使用指向一个页面的超链接来度量其威望（热门度），使用连接到网页的网页数量度量威望（一人一票）但是，外部超链接常指向某网站的根网页, 网站内部网页可能被错误的认为是不热门的
>     
>     - （2） 针对网站进行威望度量，而非针对每个网页但是“站点”概念难以定义，URL 前缀如sjtu.edu.cn包含威望不同的涉及多个主题的网页
>   
>   - 改进方法：
>     
>     - PageRank 威望度的传递，当利用网页的超链接来计算威望时，对来自本身具有高威望的网站/页的链接赋予较大权重（基于Google搜索引擎的排名方法 PageRank）（1）循环定义（2）建立并求解联立线性方程组。此思想来自社交网络理论：**定义人的威望**
>     
>     - HITS算法：基于链接中心hub和权威页authority的排名
>       • 链接中心：包含许多指向某主题相关网页的链接的网页
>       • 权威页：包含某主题的实际信息的网页
>       
>       - 基于所指向的authorities的威望来计算每个网页的链接中心威望，基于指向它的hub的威望来计算每个网页的权威页威望。
>       
>       - 像以前一样，威望的定义是循环的，并且是由线性联立方程组定义的，使用权威页威望对查询结果进行排序。
> 
> - 4 文档索引
>   
>   - 倒排索引可以记录：
>     • 关键词在文档中的位置：以允许基于接近度的排位
>     • 关键词的TF和DF：以允许基于TF-IDF的排位
>   
>   - and运算：找出包含所有K1, K2, ..., Kn的文档
>     • 交集：S1∩S2∩…∩Sn
>   
>   - or运算：至少包含一个K1, K2, …, Kn的文档
>     • 并集：S1∪S2∪…∪Sn
>   
>   - 保持每个Si是有序的以允许通过归并实现高效的交集/并集运算，
>     “not” 也能通过归并有序列表高效实现。
> 
> - 5 检索效果的质量
>   
>   - IR系统通过使用只支持近似检索的索引结构来节省空间，导致：
>     • 误弃(false negative或false drop)：某些相关文档未被检索到
>     • 误选(false positive) – 某些不相关文档可能被检索到
>   
>   - 对许多应用来说，一个好的索引应该**不允许任何误弃，但是可以允许一些误选**。
>   
>   - IR系统的性能指标：
>     
>     - 查准率(precision)：检索到的文档中与查询相关的文档所占百分比
>     
>     - 查全率(recall)：与查询相关的文档中被检索出来的百分比
>   
>   - 查全率与查准率的权衡：可通过检索许多文档（调低相关度）来增加查全率，但是会取得许多不相关文档，从而降低查准率。
>   
>   - 检索有效性的度量：
>     • 将查全率作为取得的文档数目的函数来度量；
>     • 查准率作为查全率的函数来度量，等价地，也是取得的文档数目的函数。• 例如“查全率为50%时查准率为75%，查全率为75%时查准率为60%
>   
>   - 如何定义哪些文档实际上是相关的，哪些不是？可以尝试在手动标注的数据集上进行验证。

Web信息检索

> - Web爬虫
>   
>   - 定义：网络爬虫Web crawlers是定位和收集Web上的信息的程序
>     
>     - 从一个种子文档集合开始，沿着已知文档中存在的超文本链接递归地找到其他文档
>     
>     - 取得的文档交给一个索引系统，索引后可以丢弃，或保存为缓存副本
>   
>   - 过程：
>     
>     - 爬遍整个Web耗时，搜索引擎通常只覆盖Web的一部分，而非全部。
>     
>     - 爬虫是由多台机器上的多个进程并行进行的。
>     
>     - 索引系统也运行在多个机器上
> 
> - 网页排名之外
>   
>   - 原本被设计为寻找与一个查询请求相关联的文本文档
>   
>   - **信息抽取系统将文本形式的信息转换成结构化形式**：
>     • 从文本房产广告中抽取房屋属性（大小, 地址, 卧室个数等）
>     • 从一篇文章中抽取主题和人名
>   
>   - 用关系或XML来存储抽取的结构数据，系统寻找数据间的关联来回答查询
>   
>   - 问答系统试图对用户提出的问题提供直接的答案。

目录和分类

> - 1 图书馆将相关文档存储在一起以便利浏览，用户不仅可以看到所要的文档, 还可看到相关的文档。因此，分类系统将逻辑上相关的文档组织在一起以便于浏览。
>   
>   - 组织是层次式的：分类层次
>   
>   - 图书馆系统的一个分类层次：
>     
>     ![](大数据.assets/2023-06-25-21-03-17-image.png)
> 
> - 2 分类DAG：在信息检索系统中，文档可以保存到分类层次中的不同位置上，因为文档储存的物理位置并不重要。分类层次是个有向无圈图 (DAG)。
>   
>   - 图书馆信息检索系统的分类DAG如图
>     
>     ![](大数据.assets/2023-06-25-21-03-52-image.png)
> 
> - 3 Web目录就是网页的分类目录，例如 Yahoo! 目录，Open Directory项目Ø
>   
>   - 组织一个目录结构需要面临的任务：
>     （1） 目录层次应该是怎样的？
>     （2） 给定一个文档，确定哪些目录节点是与该文档相关的类别Ø 解决方法经常是手动的，或基于词项相似性将文档分类到分类层次中。

## 9 知识图谱

### 什么是知识图谱

概念

> - 基于图的数据结构
>   
>   - 实体、属性和关系
>   
>   - 节点是实体
>   
>   - 节点会被一些属性标注
>   
>   - 两个节点之间的边代表关系
>     
>     ![](大数据.assets/2023-06-25-22-05-50-image.png)

为什么使用知识图谱：

> - 原因：
>   
>   - 许多AI任务的关键要素
>   
>   - 从数据到人类语义的桥梁
>   
>   - 在图形分析上已经有数十年的研究
>   
>   - 知识图谱可用来更好地查询复杂的关联信息，从语义层面理解用户意图，改进搜索质量
> 
> - 以下是一些知识图谱的产品：
>   
>   - Google Knowledge Graph
>     • Google Knowledge Vault
>     • Amazon Product Graph
>     • Facebook Graph API
>     • IBM Watson
>     • Microsoft Satori
>     • Project Hanover/Literome
>     • LinkedIn Knowledge Graph

知识图谱的来源

> 来源于有信息的地方：
> • 结构化文本 : Wikipedia Infoboxes, tables, databases, social nets
> • 非结构化文本 : WWW, news, social media, reference articles
> • 图片
> • 视频 : YouTube, video feeds

知识表达的选择

> - 1 RDF（Resource Description Framework）：大多数知识图谱都是用RDF三元组来表示的，RDF是一种处理元数据的应用。（元数据是指描述数据的数据或者说是描述信息的信息）
>   
>   - 例如: 书的内容是书的数据，作者的名字、出版社、地址是书的元数据
>   
>   - **RDF的基本构造**：陈述(或者叫做声明，statement)了一个资源-资源具有的属性(attribute)-属性值(value) (即，subject-predicate/relation-object)的三元组。每一个被描述的资源拥有一个统一资源标识符(URI)
>   
>   - RDF的**两种断言表达方式**：ABox (assertions) versus TBox (terminology)：
>     
>     - Tbox是关于概念术语的断言 ，Abox是关于个体的断言
>     
>     - Tbox声明概念和角色间的包含关系，而Abox是关于个体的实例断言集合，断言包括声明个体是某概念的实例，以及个体之间的二元关系。
>     
>     - 例如：Abox -> A is an instance of B / John is a Person
>       Tbox -> All Students are Persons / There are two types of Persons: Students and Teachers
> 
> - 2 OWL（Web Ontology Language）：
>   
>   - RDF是领域无关的,只能表示二元谓词（连接两个客体的谓词就叫二元谓词)。
>   
>   - 不足以支持web上的复杂应用，因此W3C又发展了Web本体语言(OWL)，
>   
>   - OWL是RDF的扩展，有相同的语法结构，可以定义词汇之间的关系，类与类的关系，属性与属性之间的关系等等
> 
> - 3 语义网络 Semantic Web
>   
>   - 通过提升描述信息的语义本体以及规范化程度来支持更加方便迅速和智能化的信息集成、聚合与融合。
>   
>   - 被标注的数据可以为自动化的一些操作提供关键的资源。
>   
>   - 但是这一点也是它的**弱点所在**，对于大量的不标准的语义表达，很难标注。
> 
> - 4 文本中的信息抽取：从没有标注的文本中抽取出结构化的知识
>   
>   - 面对的主要挑战有：
>     
>     - • 分块Chunking
>     
>     - • 多义词/词义消歧polysemy/word sense disambiguation
>     
>     - • 实体共指entity coreference
>     
>     - • 关系抽取relational extraction
>   
>   - 举例：解析 "猴子喜欢吃香蕉。"
>     POS Tagging == "猴子/NR 喜欢/VV 吃/VV 香蕉/NN 。/PU"
>     Chunking == "(NP 猴子)(VP 喜欢吃香蕉)"

面对的主要问题

> - 如何定义实体、属性、关系，针对以上三个问题，主要从两个方面进行研究：
>   （1）知识抽取：（对非结构化的文本）
>   ① 什么是实体：命名实体识别，实体共指
>   ② 什么是属性: 命名实体识别
>   ③ 什么是关系： 关系抽取，语义角色标注
>   （2）知识图谱的构造：（已有的知识图谱上增加边和点）
>   ① 什么是实体：实体链接，实体消歧
>   ② 什么是属性: 协作分类
>   ③ 什么是关系： 链接预测

### 文本中的知识抽取

什么是知识抽取

> - 知识抽取就是将非结构化的，有歧义的大量文本转化为结构化的精确可操作的并且试用于特定任务的知识
>   
>   - 知识的抽取需要运用到自然语言处理（NLP）技术，我们可以得到分词和词性标注，并给命名实体做标识比如人和地名
>   
>   - 借助这些中间产物进而得到
>   
>   ![](大数据.assets/2023-06-25-22-24-18-image.png)
> 
> - 更详细的信息抽取的过程分为三步，前两步之前已经介绍了：
>   
>   - 第一步是句子层面的信息处理，可以用NLP的处理技术：句法分析中的依赖解析（Dependency Parsing），词性标记（POS tagging），命名实体识别（Named Entity Recognition）
>   
>   - 第二步是从大量的文本中查找跟识别出的信息相关的知识，使用语义分析中的指代消解（Coreference Resolution）
>   
>   - （重点）第三步是基于前两步的信息的抽取：转化为实体属性和关系。用到的技术有实体消歧（Entity Resolution），实体链接（Entity Linking）和关系抽取（Relation Extraction）
>     
>     ![](大数据.assets/2023-06-25-22-26-12-image.png)

实体链接

> - 背景：互联网网页，如新闻、博客等内容里涉及大量实体。实体链接到相应的知识库词条上，为读者提供更详尽的背景材料。这种做法实际上将互联网网页与实体之间建立了链接关系，因此被称为实体链接。手工建立实体链接关系非常费力，因此如何让计算机自动实现实体链接，成为知识图谱得到大规模应用的重要技术前提。
> 
> - 实体链接的主要任务有两个：
>   • 实体识别（Entity Recognition）
>   • 实体消歧（Entity Resolution）
> 
> - 目的：实体识别旨在从文本中发现命名实体，最典型的包括人名、地名、机构名等三类实体。近年来，人们开始尝试识别更丰富的实体类型，如电影名、产品名，等等。此外，由于知识图谱不仅涉及实体，还有大量概念（concept），因此也有研究者提出对这些概念进行识别。实体链接并不局限于文本与实体之间，如下图所示，还可以包括图像、社交媒体等数据与实体之间的关联。

关系抽取

> - 定义：从互联网网页文本中抽取实体关系。关系抽取是一种典型的信息抽取任务
>   
>   - 模板抽取：典型的开放信息抽取方法采用自举（bootstrapping）的思想，按照“模板生成实例抽取”的流程不断迭代直至收敛。
>     
>     - 比如： X是Y的首都， 可以抽取出(中国，首都，北京)、(美国，首都，华盛顿)等三元组实例 • 根据以上三元组实例可以发现更多的匹配模板，如“Y的首都是X”、“X是Y的政治中心”等等 （一开始人定义模板，后面发现更多模板）
>   
>   - 关系分类：还可以将所有关系看做分类标签，把关系抽取转换为对实体对的关系分类问题。
> 
> - 模板生成实例抽取：通过反复迭代不断抽取新的实例与模板。这种方法直观有效，但也面临很多挑战性问题，如在扩展过程中**很容易引入噪音实例与模板**，**出现语义漂移**现象，降低抽取准确率
>   
>   - 如何对这些自动发现的关系进行聚类规约是一个挑战性问题。
>     • 例如，我们通过句法分析，可以从文本中发现“华为”与“深圳”的如下关系：(华为，总部位于，深圳)、(华为，总部设置于，深圳)、以及(华为，将其总部建于，深圳)。
>     
>     • 上述发现的“总部位于”、“总部设置于”以及“将其总部建于”等三个关系实际上是同一种关系
> 
> - 关系分类：
>   
>   - 定义：将所有关系看做分类标签，把关系抽取转换为对实体对的关系分类问题。这种关系抽取方案的主要挑战在于缺乏标注语料，或者可以采用**远程监督**
>   
>   - 远程监督distant supervision：与自举思想面临的挑战类似，远程监督方法（认为一个句子包含苹果公司和乔布斯，就会有创始人的关系）会引入大量噪音训练样例，严重损害模型准确率。例如，对于(苹果，创始人，乔布斯)我们可以从文本中匹配以下四个句子
>     
>     ![](大数据.assets/2023-06-25-22-41-27-image.png)
> 
> - 总结：关系抽取是知识图谱构建的核心技术，决定了知识图谱中知识的规模和质量。关系抽取是知识图谱研究的热点问题，还有很多挑战性问题需要解决：•从高噪音的互联网数据中抽取关系的鲁棒性，扩大抽取关系的类型与抽取知识的覆盖面，等等

### 知识图谱的应用

知识推理

> - 定义：推理能力是人类智能的重要特征，能够从已有知识中发现隐含知识。推理往往需要相关规则的支持，例如从“妻子的父亲”推理出“岳父”，从出生日期和当前时间推理出年龄，等等。
> 
> - 问题：这些规则可以通过人们手动总结构建，但往往费时费力。很难穷举复杂关系图谱中的所有推理规则。因此，很多人研究如何自动挖掘相关推理规则或模式。目前主要依赖关系之间的同现情况，利用关联挖掘技术来自动发现推理规则
> 
> - 做法：实体关系之间存在丰富的同现信息
>   
>   - 知识推理可以用于发现实体间新的关系。例如，根据“父亲+父亲=>祖父”的推理规则，如果两实体间存在“父亲+父亲”的关系路径，我们就可以推理它们之间存在“祖父”的关系。
>     
>     ![](大数据.assets/2023-06-25-23-26-06-image.png)

知识表示    

> - 最近，伴随着深度学习和表示学习的革命性发展，研究者也开始探索面向知识图谱的表示学习方案。
> 
> - 基本思想是：将知识图谱中的实体和关系的语义信息用低维向量表示。其中，最简单有效的模型是最近提出的TransE：
>   
>   - TransE基于实体和关系的分布式向量表示，将每个三元组实例（head，relation，tail）中的关系relation看做从实体head到实体tail的翻译，通过不断调整h、r和t（head、relation和tail的向量），使（h + r） 尽可能与 t 相等，即 h + r = t。该优化目标如下图所示。
>     
>     ![](大数据.assets/2023-06-25-23-27-50-image.png)

实际应用的系统

> - ConceptNet：人手工构建，众包项目
>   
>   ![](大数据.assets/2023-06-25-23-28-54-image.png)
>   
>   比如在conceptnet中搜索knowledge这个单词，会出现很多相关的知识
>   
>   ![](大数据.assets/2023-06-25-23-29-34-image.png)
> 
> - NELL：它的目标就是在挖掘网页上的非结构文本，用自动的方法来构建知识图谱。（CMU大学推动），计算置信度加入知识库上
>   
>   ![](大数据.assets/2023-06-25-23-30-28-image.png)
> 
> - Knowledge vault：它的目标就是在挖掘网页上的非结构文本，用自动的方法来构建知识图谱，Google做的，是其背后的搜索引擎，人和机器都加入
>   
>   ![](大数据.assets/2023-06-25-23-31-19-image.png)
> 
> - Open IE：它的目标就是在挖掘网页上的非结构文本，用自动的方法来构建知识图谱。比较简单粗暴，找到句子中的谓语动词，然后做知识图谱。
>   
>   ![](大数据.assets/2023-06-25-23-32-03-image.png)
